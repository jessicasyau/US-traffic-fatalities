{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# US Fatal Traffic Accidents Analysis\nThis notebook analyzes the fatal traffic accident data from year 2000 to 2020 obtained from the National Highway Traffic Safety Administration (NHTSA), specifically from the Fatality Analysis Reporting System ([FARS](https://www.nhtsa.gov/file-downloads?p=nhtsa/downloads/FARS/)).\n\nHere's the general framework we will apply to analyze this data:\n1. Load and clean the data\n2. Explore trends in the data","metadata":{}},{"cell_type":"code","source":"#Set up the environment\nimport numpy as np\nimport pandas as pd\npd.set_option(\"display.max_columns\",None)\nimport matplotlib.pyplot as plt\n%matplotlib inline \nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-03T17:36:24.132982Z","iopub.execute_input":"2022-05-03T17:36:24.133602Z","iopub.status.idle":"2022-05-03T17:36:25.253543Z","shell.execute_reply.started":"2022-05-03T17:36:24.133496Z","shell.execute_reply":"2022-05-03T17:36:25.252717Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## 1. Load and Clean the Data\nLet's import the fatal traffic accident data from 2000 to 2020. \n\n**Note**: Some of the data files contain redundant column data. For examples, some files contain both the STATE and STATENAME features:\n- STATE: numeric code that stands for one of the US states.\n- STATENAME: string denoting one of the US states.\n\nIt is unnecessary to keep both of these features (columns), so we will drop the redundant string columns and only keep the columns containing data codes.\n\nSee this [column description file](https://github.com/jessicasyau/US-traffic-fatalities.md/blob/bb3de6a8a4572fe2185fcb0094b0864fcb65038e/description_fields/0.%20Data_descriptions.md) for more details about the columns (features) in these datasets.","metadata":{}},{"cell_type":"code","source":"# Define column data types\ndtype = {\n    'STATE':'category', 'ST_CASE':'category',\n    'VE_TOTAL':'int64', 'VE_FORMS':'int64', 'PVH_INVL':'int64', \n    'PEDS':'int64', 'PERSONS':'int64', 'PERMVIT':'int64', 'PERNOTMVIT':'int64', \n    'COUNTY':'category', 'CITY':'category',\n    'DAY':'str', 'MONTH':'str' , 'YEAR':'str' , 'DAY_WEEK':'category' , 'HOUR':'int64' , 'MINUTE':'int64' , \n    'NHS':'category' , 'ROUTE':'category' , 'RUR_URB':'category' , 'FUNC_SYS':'category' , 'RD_OWNER':'category',\n    'LATITUDE':'category' , 'LONGITUD':'category' ,  \n    'SP_JUR':'category' ,  'HARM_EV':'category' , 'MAN_COLL':'category' , 'TYP_INT':'category' , \n    'WRK_ZONE':'category' ,  'REL_ROAD':'category' ,'LGT_COND':'category' , 'WEATHER':'category' , \n    'SCH_BUS':'category' , 'NOT_HOUR':'str' , 'NOT_MIN':'str' , 'ARR_HOUR':'str' , 'ARR_MIN':'str' , \n    'CF1':'category' , 'CF2':'category' , 'CF3':'category' , 'FATALS':'int64', 'DRUNK_DR':'int64' \n}\n\n# Create list of columns to keep (if they exist)\nusecols = ['STATE', 'ST_CASE', 'VE_TOTAL', 'VE_FORMS', 'PVH_INVL','PEDS', 'PERSONS', 'PERMVIT', 'PERNOTMVIT', \n    'COUNTY', 'CITY','DAY', 'MONTH', 'YEAR', 'DAY_WEEK', 'HOUR', 'MINUTE', \n    'NHS', 'ROUTE', 'RUR_URB', 'FUNC_SYS', 'RD_OWNER',\n    'LATITUDE', 'LONGITUD',  \n    'SP_JUR',  'HARM_EV', 'MAN_COLL', 'TYP_INT', 'WRK_ZONE',  'REL_ROAD','LGT_COND', 'WEATHER', \n    'SCH_BUS', 'NOT_HOUR', 'NOT_MIN', 'ARR_HOUR', 'ARR_MIN', 'CF1', 'CF2', 'CF3', 'FATALS', 'DRUNK_DR']\n\n# Load CSV files, setting ST_CASE (unique case numbers reported for each accident) as the index\nacc_2020 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2020.CSV',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2019 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2019.CSV',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2018 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2018.csv',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2017 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2017.CSV',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2016 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2016.CSV',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2015 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2015.csv',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2014 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2014.csv',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2013 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2013.csv',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2012 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2012.csv',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2011 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2011.CSV',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2010 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2010.CSV',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2009 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2009.CSV',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2008 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2008.CSV',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2007 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2007.CSV',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2006 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2006.CSV',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2005 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2005.CSV',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2004 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2004.CSV',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2003 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2003.CSV',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2002 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2002.CSV',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2001 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2001.CSV',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2000 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2000.CSV',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\n\n# print the size of each dataframe\ndf_list = [acc_2020, acc_2019, acc_2018, acc_2017, acc_2016, acc_2015, acc_2014, acc_2013, acc_2012, acc_2011, acc_2010,\n          acc_2009, acc_2008, acc_2007, acc_2006, acc_2005, acc_2004, acc_2003, acc_2002, acc_2001, acc_2000]\n\ni = 2020\nfor df in df_list:\n    print('{}: '.format(i), df.shape)\n    i-=1","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-03T17:36:27.884080Z","iopub.execute_input":"2022-05-03T17:36:27.884337Z","iopub.status.idle":"2022-05-03T17:36:39.438135Z","shell.execute_reply.started":"2022-05-03T17:36:27.884309Z","shell.execute_reply":"2022-05-03T17:36:39.437207Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Let's check to see if any of the YEAR, MONTH, or DAY columns contain missing or unknown values (i.e. -- or 99).","metadata":{}},{"cell_type":"code","source":"# Create a function that checks how many values are missing or unknown in the col_name column of df\ndef missing_val(df, col_name):\n    return df.loc[df[col_name].isin(['99','--'])].shape[0]\n\n# Create a list of dataframes and list of date and time columns to check\ndf_list = [acc_2020, acc_2019, acc_2018, acc_2017, acc_2016, acc_2015, acc_2014, acc_2013, acc_2012, acc_2011, acc_2010,\n          acc_2009, acc_2008, acc_2007, acc_2006, acc_2005, acc_2004, acc_2003, acc_2002, acc_2001, acc_2000]\n\ndatetime_list = ['YEAR', 'MONTH', 'DAY']\n\n# Generate a dictionary containing the number of missing date and time column values for each dataframe\nmissing_dict = {}\ni = 2020\nfor df in df_list:\n    missing_list = []\n    for check in datetime_list:\n        missing_list.append(missing_val(df, check))\n    missing_dict[i] = missing_list\n    i -=1\n    \nmissing_dict","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-03T17:36:42.759599Z","iopub.execute_input":"2022-05-03T17:36:42.759891Z","iopub.status.idle":"2022-05-03T17:36:43.345516Z","shell.execute_reply.started":"2022-05-03T17:36:42.759856Z","shell.execute_reply":"2022-05-03T17:36:43.344686Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Only a smart portion of DAY column values are missing from these dataframes, let's drop them.","metadata":{}},{"cell_type":"code","source":"# Drop the rows that have missing or unknown DAY values\nacc_2000 = acc_2000[acc_2000['DAY']!= '99']\nacc_2001 = acc_2001[acc_2001['DAY']!= '99']\nacc_2002 = acc_2002[acc_2002['DAY']!= '99']\nacc_2003 = acc_2003[acc_2003['DAY']!= '99']\nacc_2004 = acc_2004[acc_2004['DAY']!= '99']\nacc_2005 = acc_2005[acc_2005['DAY']!= '99']\nacc_2008 = acc_2008[acc_2008['DAY']!= '99']","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-03T17:36:46.274225Z","iopub.execute_input":"2022-05-03T17:36:46.274498Z","iopub.status.idle":"2022-05-03T17:36:46.381002Z","shell.execute_reply.started":"2022-05-03T17:36:46.274468Z","shell.execute_reply":"2022-05-03T17:36:46.380026Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Let's create a datetime column DATE that combines the YEAR, MONTH, and DAY columns.","metadata":{}},{"cell_type":"code","source":"# Create a function to combine the YEAR, MONTH, and DAY columns into a datatime column\ndef make_date(df):\n    date_temp = df['YEAR'] + '-' + df['MONTH'] + '-' + df['DAY']\n    df['DATE'] = pd.to_datetime(date_temp, format=\"%Y-%m-%d\")\n    df = df.drop(['YEAR', 'MONTH', 'DAY'], axis=1)\n    return df\n    \ndf_list = [acc_2020, acc_2019, acc_2018, acc_2017, acc_2016, acc_2015, acc_2014, acc_2013, acc_2012, acc_2011, acc_2010,\n          acc_2009, acc_2008, acc_2007, acc_2006, acc_2005, acc_2004, acc_2003, acc_2002, acc_2001, acc_2000]\n\nacc_2000 = make_date(acc_2000)\nacc_2001 = make_date(acc_2001)\nacc_2002 = make_date(acc_2002)\nacc_2003 = make_date(acc_2003)\nacc_2004 = make_date(acc_2004)\nacc_2005 = make_date(acc_2005)\nacc_2006 = make_date(acc_2006)\nacc_2007 = make_date(acc_2007)\nacc_2008 = make_date(acc_2008)\nacc_2009 = make_date(acc_2009)\nacc_2010 = make_date(acc_2010)\nacc_2011 = make_date(acc_2011)\nacc_2012 = make_date(acc_2012)\nacc_2013 = make_date(acc_2013)\nacc_2014 = make_date(acc_2014)\nacc_2015 = make_date(acc_2015)\nacc_2016 = make_date(acc_2016)\nacc_2017 = make_date(acc_2017)\nacc_2018 = make_date(acc_2018)\nacc_2019 = make_date(acc_2019)\nacc_2020 = make_date(acc_2020)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T17:36:47.790017Z","iopub.execute_input":"2022-05-03T17:36:47.790541Z","iopub.status.idle":"2022-05-03T17:36:48.478684Z","shell.execute_reply.started":"2022-05-03T17:36:47.790484Z","shell.execute_reply":"2022-05-03T17:36:48.477712Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# 2. Explore Trends in the Data\nLet's first determine the list of features (columns) that are in common between all dataframes.","metadata":{}},{"cell_type":"code","source":"df_list = [acc_2020, acc_2019, acc_2018, acc_2017, acc_2016, acc_2015, acc_2014, acc_2013, acc_2012, acc_2011, acc_2010,\n          acc_2009, acc_2007, acc_2008, acc_2006, acc_2005, acc_2004, acc_2003, acc_2002, acc_2001, acc_2000]\n\n# Create a list of column lists for each dataframe\ncols_list = []\ni = 2020\nfor df in df_list:\n    cols_list.append(set(df.columns))\n\n# Find the intersection between all of those column lists\nshared_features = cols_list[0].intersection(*cols_list)\nprint(shared_features)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-03T17:36:52.356005Z","iopub.execute_input":"2022-05-03T17:36:52.356282Z","iopub.status.idle":"2022-05-03T17:36:52.376513Z","shell.execute_reply.started":"2022-05-03T17:36:52.356251Z","shell.execute_reply":"2022-05-03T17:36:52.375598Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"From the list of shared features among all of the datasets, here are the ones that I would like to explore:\n- DAY_WEEK: day of the week\n- WEATHER: weather condition\n- STATE: which state the accident occured in\n- HOUR: the time (number of hours from midnight) at which the accident occured\n- HARM_EV: the first injury or damage producing event of the accident\n- SP_JUR: whether the accident occured in a location classified as a special jurisdiction (e.g. military, indian reserve, campus, etc.)\n- SCH_BUS: whether a school bus was involved in the accident\n- LGT_COND: lighting condition","metadata":{}},{"cell_type":"markdown","source":"### Day of the Week VS Accidents","metadata":{}},{"cell_type":"code","source":"df_list = [acc_2020, acc_2019, acc_2018, acc_2017, acc_2016, acc_2015, acc_2014, acc_2013, acc_2012, acc_2011, acc_2010,\n          acc_2009, acc_2007, acc_2008, acc_2006, acc_2005, acc_2004, acc_2003, acc_2002, acc_2001, acc_2000]\n\nseries_list = []\n\n# Create a list of dataframes containing the counts for DAY_WEEK for each year\nfor df in df_list:\n    series_list.append(df['DAY_WEEK'].value_counts().rename_axis('unique_values').to_frame('counts'))\n\n# Create a dataframe containing the counts for each day of the week\ndayweek_df=pd.DataFrame()\nfor i in range(21):\n    dayweek_df=dayweek_df.join(series_list[i], how='outer',rsuffix=\"%d\" %(20-i))\n\n# Plot the counts for each day of the week for all the dataframes from 2000 to 2020    \ni=2020\nplt.figure(figsize=(14,6))\nfor col in dayweek_df.columns:\n    ax=sns.lineplot(data=dayweek_df[\"{}\".format(col)], label=\"{}\".format(i))\n    i-=1\nax.set(ylim=(0,8000))    ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-03T16:51:47.728624Z","iopub.execute_input":"2022-05-03T16:51:47.729389Z","iopub.status.idle":"2022-05-03T16:51:48.767017Z","shell.execute_reply.started":"2022-05-03T16:51:47.729345Z","shell.execute_reply":"2022-05-03T16:51:48.765975Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"There appears to be a relationship between day of the week and accidents.","metadata":{}},{"cell_type":"markdown","source":"### Weather VS Accidents","metadata":{}},{"cell_type":"code","source":"df_list = [acc_2020, acc_2019, acc_2018, acc_2017, acc_2016, acc_2015, acc_2014, acc_2013, acc_2012, acc_2011, acc_2010,\n          acc_2009, acc_2007, acc_2008, acc_2006, acc_2005, acc_2004, acc_2003, acc_2002, acc_2001, acc_2000]\n\nseries_list = []\n\n# Create a list of dataframes containing the counts for WEATHER for each year\nfor df in df_list:\n    series_list.append(df['WEATHER'].value_counts().rename_axis('unique_values').to_frame('counts'))\n\n# Create a dataframe containing the counts for each weather condition\nweather_df=pd.DataFrame()\nfor i in range(21):\n    weather_df=weather_df.join(series_list[i], how='outer',rsuffix=\"%d\" %(20-i))\n\n# Plot the counts for each weather condition for all the dataframes from 2000 to 2020\nweather_df['weather_cond']=weather_df.index\nweather = pd.melt(weather_df, id_vars=\"weather_cond\", var_name=\"year\", value_name=\"number of accidents\")\nsns.catplot(x='weather_cond', y='number of accidents', hue='year', data=weather, kind='bar',height=6, aspect=2)\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-03T16:51:55.008889Z","iopub.execute_input":"2022-05-03T16:51:55.009152Z","iopub.status.idle":"2022-05-03T16:51:57.476028Z","shell.execute_reply.started":"2022-05-03T16:51:55.009124Z","shell.execute_reply":"2022-05-03T16:51:57.475260Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"There appears to be a relationship between weather and accident. However, the correlation between clear weather (code 1) and accidents may be due to the high frequency of clear weather days. Further investigation is required.","metadata":{}},{"cell_type":"markdown","source":"### State VS Accidents","metadata":{}},{"cell_type":"code","source":"df_list = [acc_2020, acc_2019, acc_2018, acc_2017, acc_2016, acc_2015, acc_2014, acc_2013, acc_2012, acc_2011, acc_2010,\n          acc_2009, acc_2007, acc_2008, acc_2006, acc_2005, acc_2004, acc_2003, acc_2002, acc_2001, acc_2000]\n\nseries_list = []\n\n# Create a list of dataframes containing the counts for STATE for each year\nfor df in df_list:\n    series_list.append(df['STATE'].value_counts().rename_axis('unique_values').to_frame('counts'))\n\n# Create a dataframe containing the counts for each state\nstate_df=pd.DataFrame()\nfor i in range(21):\n    state_df=state_df.join(series_list[i], how='outer',rsuffix=\"%d\" %(20-i))\n\n# Plot the counts for each state for all the dataframes from 2000 to 2020    \ni=2020\nplt.figure(figsize=(20,6))\nfor col in state_df.columns:\n    ax=sns.lineplot(data=state_df[\"{}\".format(col)], label=\"{}\".format(i))\n    i-=1\n   ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-03T16:52:06.434378Z","iopub.execute_input":"2022-05-03T16:52:06.434676Z","iopub.status.idle":"2022-05-03T16:52:07.933601Z","shell.execute_reply.started":"2022-05-03T16:52:06.434631Z","shell.execute_reply":"2022-05-03T16:52:07.932640Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"There appears to be a relationship between the State and the number of accidents.","metadata":{}},{"cell_type":"markdown","source":"### Time VS Accidents","metadata":{}},{"cell_type":"markdown","source":"Let's examine the data to see if there are any patterns between the hour at which the accidents occur on weekdays (Monday to Friday; code: 2-6)","metadata":{}},{"cell_type":"code","source":"df_list = [acc_2020, acc_2019, acc_2018, acc_2017, acc_2016, acc_2015, acc_2014, acc_2013, acc_2012, acc_2011, acc_2010,\n          acc_2009, acc_2007, acc_2008, acc_2006, acc_2005, acc_2004, acc_2003, acc_2002, acc_2001, acc_2000]\n\nseries_list = []\n\n# Create a list of dataframes containing the counts for HOUR on weekdays (DAY_WEEK= 2-6) for each year\nfor df in df_list:\n    weekday_df = df[df['DAY_WEEK'].isin(['2','3','4','5','6'])]\n    series_list.append(weekday_df['HOUR'].value_counts().rename_axis('unique_values').to_frame('counts'))\n\n# Create a dataframe containing the counts for each hour\nhour_df=pd.DataFrame()\nfor i in range(21):\n    hour_df=hour_df.join(series_list[i], how='outer',rsuffix=\"%d\" %(20-i))\n\n    \n# Remove the value 99 (unknown) from hour_df\nhour_df = hour_df[hour_df.index!=99]\n\n# Plot the counts for each hour for all the dataframes from 2000 to 2020    \ni=2020\nplt.figure(figsize=(20,6))\nfor col in hour_df.columns:\n    ax=sns.lineplot(data=hour_df[\"{}\".format(col)], label=\"{}\".format(i))\n    ax.set_xlim(0,24)\n    ax.set_xticks(range(1,25))\n    i-=1","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-03T16:52:11.162631Z","iopub.execute_input":"2022-05-03T16:52:11.163727Z","iopub.status.idle":"2022-05-03T16:52:12.568965Z","shell.execute_reply.started":"2022-05-03T16:52:11.163666Z","shell.execute_reply":"2022-05-03T16:52:12.568080Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Let's examine the data to see if there are any patterns between the hour at which the accidents occur on weekends (Sunday and Saturday; code 1 and 7)","metadata":{}},{"cell_type":"code","source":"df_list = [acc_2020, acc_2019, acc_2018, acc_2017, acc_2016, acc_2015, acc_2014, acc_2013, acc_2012, acc_2011, acc_2010,\n          acc_2009, acc_2007, acc_2008, acc_2006, acc_2005, acc_2004, acc_2003, acc_2002, acc_2001, acc_2000]\n\nseries_list = []\n\n# Create a list of dataframes containing the counts for HOUR on weekends (DAY_WEEK= 1 or 7) for each year\nfor df in df_list:\n    weekend_df = df[df['DAY_WEEK'].isin(['1','7'])]\n    series_list.append(weekend_df['HOUR'].value_counts().rename_axis('unique_values').to_frame('counts'))\n\n# Create a dataframe containing the counts for each hour\nhour_df=pd.DataFrame()\nfor i in range(21):\n    hour_df=hour_df.join(series_list[i], how='outer',rsuffix=\"%d\" %(20-i))\n\n    \n# Remove the value 99 (unknown) from hour_df\nhour_df = hour_df[hour_df.index!=99]\n\n# Plot the counts for each hour for all the dataframes from 2000 to 2020    \ni=2020\nplt.figure(figsize=(20,6))\nfor col in hour_df.columns:\n    ax=sns.lineplot(data=hour_df[\"{}\".format(col)], label=\"{}\".format(i))\n    ax.set_xlim(0,24)\n    ax.set_xticks(range(1,25))\n    i-=1","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-03T16:52:20.013521Z","iopub.execute_input":"2022-05-03T16:52:20.013830Z","iopub.status.idle":"2022-05-03T16:52:21.243352Z","shell.execute_reply.started":"2022-05-03T16:52:20.013798Z","shell.execute_reply":"2022-05-03T16:52:21.242481Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"There appears to be a relationship between the time of the day (HOUR) and the number of accidents:\n- On weekdays:\n    - Most accidents occur between 3pm-10pm, which some peaks from 3pm-4pm, 5pm-6pm, and 6pm-7pm\n    - There are also peaks from 6-8am and from 2-3am\n- On weekends:\n    - There is a peak from 2am-3am\n    - There is a steady increase from 10am to 6pm, then it stays pretty steady after that\n\nIn conclusion, the time of day does seem to correlate with the number of accidents.","metadata":{}},{"cell_type":"markdown","source":"### First Harmful Event VS Accidents","metadata":{}},{"cell_type":"code","source":"df_list = [acc_2020, acc_2019, acc_2018, acc_2017, acc_2016, acc_2015, acc_2014, acc_2013, acc_2012, acc_2011, acc_2010,\n          acc_2009, acc_2007, acc_2008, acc_2006, acc_2005, acc_2004, acc_2003, acc_2002, acc_2001, acc_2000]\n\nseries_list = []\n\n# Create a list of dataframes containing the counts for WEATHER for each year\nfor df in df_list:\n    series_list.append(df['HARM_EV'].value_counts().rename_axis('unique_values').to_frame('counts'))\n\n# Create a dataframe containing the counts for each weather condition\nharmful_df=pd.DataFrame()\nfor i in range(21):\n    harmful_df=harmful_df.join(series_list[i], how='outer',rsuffix=\"%d\" %(20-i))\n\n# Plot the counts for each weather condition for all the dataframes from 2000 to 2020\nharmful_df['thing']=harmful_df.index\nharmful = pd.melt(harmful_df, id_vars=\"thing\", var_name=\"year\", value_name=\"number of accidents\")\nplt.figure(figsize=(20,6))\nsns.catplot(x='thing', y='number of accidents', hue='year', data=harmful, kind='bar',height=6,aspect=3)\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-03T16:52:27.349834Z","iopub.execute_input":"2022-05-03T16:52:27.350609Z","iopub.status.idle":"2022-05-03T16:52:35.869487Z","shell.execute_reply.started":"2022-05-03T16:52:27.350568Z","shell.execute_reply":"2022-05-03T16:52:35.868676Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"The most common first injury or damage producing events are:\n1. motor vehicle in transport (code: 12)\n2. pedestrian (code: 8)\n3. rollover/overturn (code: 1)\n4. tree (code: 42)\n5. curb (code: 33)\n6. ditch (code: 34)\n7. utility/light pole (code: 30)\n8. guardrail face (code: 24)\n9. pedalcyclist (code: 9)\n10. embankment (code: 35)\n\n**clarification**: these are the things that the vehicle first came into contact with in the accident. These things did not necessarily cause the accident.\n\nAlthough there may appear to be a pattern here, further investigation is required to determine if the availability of each thing on the HARM_EV list contributes to the frequency we see here.","metadata":{}},{"cell_type":"markdown","source":"### Special Jurisdiction VS Accidents","metadata":{}},{"cell_type":"code","source":"df_list = [acc_2020, acc_2019, acc_2018, acc_2017, acc_2016, acc_2015, acc_2014, acc_2013, acc_2012, acc_2011, acc_2010,\n          acc_2009, acc_2007, acc_2008, acc_2006, acc_2005, acc_2004, acc_2003, acc_2002, acc_2001, acc_2000]\n\nseries_list = []\n\n# Create a list of dataframes containing the counts for SP_JUR for each year\nfor df in df_list:\n    series_list.append(df['SP_JUR'].value_counts().rename_axis('unique_values').to_frame('counts'))\n\n# Create a dataframe containing the counts for each special jurisdiction\nspjur_df = pd.DataFrame()\nfor i in range(21):\n    spjur_df=spjur_df.join(series_list[i], how='outer',rsuffix=\"%d\" %(20-i))\n\n# Plot the counts for each special jurisdiction for all the dataframes from 2000 to 2020\nspjur_df['special jurisdiction']=spjur_df.index\nspjur = pd.melt(spjur_df, id_vars=\"special jurisdiction\", var_name=\"year\", value_name=\"number of accidents\")\nplt.figure(figsize=(20,6))\nsns.catplot(x='special jurisdiction', y='number of accidents', hue='year', data=spjur, kind='bar',height=6,aspect=3)\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-03T16:52:39.327721Z","iopub.execute_input":"2022-05-03T16:52:39.328326Z","iopub.status.idle":"2022-05-03T16:52:40.757013Z","shell.execute_reply.started":"2022-05-03T16:52:39.328286Z","shell.execute_reply":"2022-05-03T16:52:40.755942Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Majority of accidents did not occur in a special jurisdiction. Of the small fraction of accidents that did occur in a special jurisdiction, the most frequent special jurisdiction these accidents occured in was an Indian reserve, followed by a national park service.\n\nThe relationship between accidents and special jurisdictions does not seem strong enough.","metadata":{}},{"cell_type":"markdown","source":"### School Bus VS Accidents","metadata":{}},{"cell_type":"code","source":"df_list = [acc_2020, acc_2019, acc_2018, acc_2017, acc_2016, acc_2015, acc_2014, acc_2013, acc_2012, acc_2011, acc_2010,\n          acc_2009, acc_2007, acc_2008, acc_2006, acc_2005, acc_2004, acc_2003, acc_2002, acc_2001, acc_2000]\n\nseries_list = []\n\n# Create a list of dataframes containing the counts for SCH_BUS for each year\nfor df in df_list:\n    series_list.append(df['SCH_BUS'].value_counts().rename_axis('unique_values').to_frame('counts'))\n\n# Create a dataframe containing the counts for whether a school bus was involved\nschbus_df=pd.DataFrame()\nfor i in range(21):\n    schbus_df=schbus_df.join(series_list[i], how='outer',rsuffix=\"%d\" %(20-i))\n\n# Plot the counts for whether a school bus was involved for all the dataframes from 2000 to 2020\nschbus_df['school bus']=schbus_df.index\nschbus = pd.melt(schbus_df, id_vars=\"school bus\", var_name=\"year\", value_name=\"number of accidents\")\nplt.figure(figsize=(20,6))\nsns.catplot(x='school bus', y='number of accidents', hue='year', data=schbus, kind='bar',height=6,aspect=3)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-03T16:52:49.938416Z","iopub.execute_input":"2022-05-03T16:52:49.938739Z","iopub.status.idle":"2022-05-03T16:52:50.873029Z","shell.execute_reply.started":"2022-05-03T16:52:49.938706Z","shell.execute_reply":"2022-05-03T16:52:50.872289Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"The majority of accidents did not involve a school bus.","metadata":{}},{"cell_type":"markdown","source":"### Lighing Condition VS Accidents","metadata":{}},{"cell_type":"code","source":"df_list = [acc_2020, acc_2019, acc_2018, acc_2017, acc_2016, acc_2015, acc_2014, acc_2013, acc_2012, acc_2011, acc_2010,\n          acc_2009, acc_2007, acc_2008, acc_2006, acc_2005, acc_2004, acc_2003, acc_2002, acc_2001, acc_2000]\n\nseries_list = []\n\n# Create a list of dataframes containing the counts for LGT_COND for each year\nfor df in df_list:\n    series_list.append(df['LGT_COND'].value_counts().rename_axis('unique_values').to_frame('counts'))\n\n# Create a dataframe containing the counts for each lighting condition\nlgtcond_df=pd.DataFrame()\nfor i in range(21):\n    lgtcond_df=lgtcond_df.join(series_list[i], how='outer',rsuffix=\"%d\" %(20-i))\n\n# Plot the counts for each lighting condition for all the dataframes from 2000 to 2020\nlgtcond_df['lighting']=lgtcond_df.index\nlgtcond = pd.melt(lgtcond_df, id_vars=\"lighting\", var_name=\"year\", value_name=\"number of accidents\")\nplt.figure(figsize=(20,6))\nsns.catplot(x='lighting', y='number of accidents', hue='year', data=lgtcond, kind='bar',height=6,aspect=3)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T16:52:55.140845Z","iopub.execute_input":"2022-05-03T16:52:55.141105Z","iopub.status.idle":"2022-05-03T16:52:56.703419Z","shell.execute_reply.started":"2022-05-03T16:52:55.141078Z","shell.execute_reply":"2022-05-03T16:52:56.702752Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"The majority of accidents seem to have occured during daylight (code=1), followed by Dark or not lighted (code=2), then Dark but lighted (code=3), then Dusk (code=5) and Dawn (code=4).\n\nHowever, this pattern may be influenced by the time most cars are on the road. For example, perhaps most people drive during daylight, so the proportion of accidents that occur during daylight would be higher. Further investigation is required.","metadata":{}}]}