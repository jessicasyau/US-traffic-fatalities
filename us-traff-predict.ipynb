{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Set up the environment\nimport numpy as np\nimport pandas as pd\npd.set_option(\"display.max_columns\",None)\nimport matplotlib.pyplot as plt\n%matplotlib inline \nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-05-03T18:07:12.754606Z","iopub.execute_input":"2022-05-03T18:07:12.755092Z","iopub.status.idle":"2022-05-03T18:07:14.000383Z","shell.execute_reply.started":"2022-05-03T18:07:12.754948Z","shell.execute_reply":"2022-05-03T18:07:13.998878Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Define column data types\ndtype = {\n    'STATE':'category', 'ST_CASE':'category',\n    'VE_TOTAL':'int64', 'VE_FORMS':'int64', 'PVH_INVL':'int64', \n    'PEDS':'int64', 'PERSONS':'int64', 'PERMVIT':'int64', 'PERNOTMVIT':'int64', \n    'COUNTY':'category', 'CITY':'category',\n    'DAY':'str', 'MONTH':'str' , 'YEAR':'str' , 'DAY_WEEK':'category' , 'HOUR':'int64' , 'MINUTE':'int64' , \n    'NHS':'category' , 'ROUTE':'category' , 'RUR_URB':'category' , 'FUNC_SYS':'category' , 'RD_OWNER':'category',\n    'LATITUDE':'category' , 'LONGITUD':'category' ,  \n    'SP_JUR':'category' ,  'HARM_EV':'category' , 'MAN_COLL':'category' , 'TYP_INT':'category' , \n    'WRK_ZONE':'category' ,  'REL_ROAD':'category' ,'LGT_COND':'category' , 'WEATHER':'category' , \n    'SCH_BUS':'category' , 'NOT_HOUR':'str' , 'NOT_MIN':'str' , 'ARR_HOUR':'str' , 'ARR_MIN':'str' , \n    'CF1':'category' , 'CF2':'category' , 'CF3':'category' , 'FATALS':'int64', 'DRUNK_DR':'int64' \n}\n\n# Create list of columns to keep (if they exist)\nusecols = ['STATE', 'ST_CASE', 'VE_TOTAL', 'VE_FORMS', 'PVH_INVL','PEDS', 'PERSONS', 'PERMVIT', 'PERNOTMVIT', \n    'COUNTY', 'CITY','DAY', 'MONTH', 'YEAR', 'DAY_WEEK', 'HOUR', 'MINUTE', \n    'NHS', 'ROUTE', 'RUR_URB', 'FUNC_SYS', 'RD_OWNER',\n    'LATITUDE', 'LONGITUD',  \n    'SP_JUR',  'HARM_EV', 'MAN_COLL', 'TYP_INT', 'WRK_ZONE',  'REL_ROAD','LGT_COND', 'WEATHER', \n    'SCH_BUS', 'NOT_HOUR', 'NOT_MIN', 'ARR_HOUR', 'ARR_MIN', 'CF1', 'CF2', 'CF3', 'FATALS', 'DRUNK_DR']\n\n# Load CSV files, setting ST_CASE (unique case numbers reported for each accident) as the index\nacc_2020 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2020.CSV',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2019 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2019.CSV',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2018 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2018.csv',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2017 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2017.CSV',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2016 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2016.CSV',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2015 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2015.csv',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2014 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2014.csv',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2013 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2013.csv',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2012 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2012.csv',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2011 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2011.CSV',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2010 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2010.CSV',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2009 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2009.CSV',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2008 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2008.CSV',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2007 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2007.CSV',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2006 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2006.CSV',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2005 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2005.CSV',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2004 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2004.CSV',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2003 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2003.CSV',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2002 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2002.CSV',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2001 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2001.CSV',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\nacc_2000 = pd.read_csv('/kaggle/input/us-traffic-accidents-in-2019/accident_2000.CSV',index_col='ST_CASE',usecols=lambda x: x in usecols,dtype=dtype)\n\n# print the size of each dataframe\ndf_list = [acc_2020, acc_2019, acc_2018, acc_2017, acc_2016, acc_2015, acc_2014, acc_2013, acc_2012, acc_2011, acc_2010,\n          acc_2009, acc_2008, acc_2007, acc_2006, acc_2005, acc_2004, acc_2003, acc_2002, acc_2001, acc_2000]\n\ni = 2020\nfor df in df_list:\n    print('{}: '.format(i), df.shape)\n    i-=1","metadata":{"execution":{"iopub.status.busy":"2022-05-03T18:07:17.129414Z","iopub.execute_input":"2022-05-03T18:07:17.129677Z","iopub.status.idle":"2022-05-03T18:07:26.064020Z","shell.execute_reply.started":"2022-05-03T18:07:17.129652Z","shell.execute_reply":"2022-05-03T18:07:26.062840Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Create a function that checks how many values are missing or unknown in the col_name column of df\ndef missing_val(df, col_name):\n    return df.loc[df[col_name].isin(['99','--'])].shape[0]\n\n# Create a list of dataframes and list of date and time columns to check\ndf_list = [acc_2020, acc_2019, acc_2018, acc_2017, acc_2016, acc_2015, acc_2014, acc_2013, acc_2012, acc_2011, acc_2010,\n          acc_2009, acc_2008, acc_2007, acc_2006, acc_2005, acc_2004, acc_2003, acc_2002, acc_2001, acc_2000]\n\ndatetime_list = ['YEAR', 'MONTH', 'DAY']\n\n# Generate a dictionary containing the number of missing date and time column values for each dataframe\nmissing_dict = {}\ni = 2020\nfor df in df_list:\n    missing_list = []\n    for check in datetime_list:\n        missing_list.append(missing_val(df, check))\n    missing_dict[i] = missing_list\n    i -=1\n    \nmissing_dict","metadata":{"execution":{"iopub.status.busy":"2022-05-03T18:07:30.523850Z","iopub.execute_input":"2022-05-03T18:07:30.524188Z","iopub.status.idle":"2022-05-03T18:07:30.918535Z","shell.execute_reply.started":"2022-05-03T18:07:30.524135Z","shell.execute_reply":"2022-05-03T18:07:30.917319Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Drop the rows that have missing or unknown DAY values\nacc_2000 = acc_2000[acc_2000['DAY']!= '99']\nacc_2001 = acc_2001[acc_2001['DAY']!= '99']\nacc_2002 = acc_2002[acc_2002['DAY']!= '99']\nacc_2003 = acc_2003[acc_2003['DAY']!= '99']\nacc_2004 = acc_2004[acc_2004['DAY']!= '99']\nacc_2005 = acc_2005[acc_2005['DAY']!= '99']\nacc_2008 = acc_2008[acc_2008['DAY']!= '99']","metadata":{"execution":{"iopub.status.busy":"2022-05-03T18:07:33.131813Z","iopub.execute_input":"2022-05-03T18:07:33.132096Z","iopub.status.idle":"2022-05-03T18:07:33.220467Z","shell.execute_reply.started":"2022-05-03T18:07:33.132068Z","shell.execute_reply":"2022-05-03T18:07:33.219692Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Create a function to combine the YEAR, MONTH, and DAY columns into a datatime column\ndef make_date(df):\n    date_temp = df['YEAR'] + '-' + df['MONTH'] + '-' + df['DAY']\n    df['DATE'] = pd.to_datetime(date_temp, format=\"%Y-%m-%d\")\n    df = df.drop(['YEAR', 'MONTH', 'DAY'], axis=1)\n    return df\n    \ndf_list = [acc_2020, acc_2019, acc_2018, acc_2017, acc_2016, acc_2015, acc_2014, acc_2013, acc_2012, acc_2011, acc_2010,\n          acc_2009, acc_2008, acc_2007, acc_2006, acc_2005, acc_2004, acc_2003, acc_2002, acc_2001, acc_2000]\n\nacc_2000 = make_date(acc_2000)\nacc_2001 = make_date(acc_2001)\nacc_2002 = make_date(acc_2002)\nacc_2003 = make_date(acc_2003)\nacc_2004 = make_date(acc_2004)\nacc_2005 = make_date(acc_2005)\nacc_2006 = make_date(acc_2006)\nacc_2007 = make_date(acc_2007)\nacc_2008 = make_date(acc_2008)\nacc_2009 = make_date(acc_2009)\nacc_2010 = make_date(acc_2010)\nacc_2011 = make_date(acc_2011)\nacc_2012 = make_date(acc_2012)\nacc_2013 = make_date(acc_2013)\nacc_2014 = make_date(acc_2014)\nacc_2015 = make_date(acc_2015)\nacc_2016 = make_date(acc_2016)\nacc_2017 = make_date(acc_2017)\nacc_2018 = make_date(acc_2018)\nacc_2019 = make_date(acc_2019)\nacc_2020 = make_date(acc_2020)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T18:07:34.576965Z","iopub.execute_input":"2022-05-03T18:07:34.577250Z","iopub.status.idle":"2022-05-03T18:07:35.196068Z","shell.execute_reply.started":"2022-05-03T18:07:34.577225Z","shell.execute_reply":"2022-05-03T18:07:35.195387Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Develop a Model\nLet's define our first model to predict the number of accidents based on date only.","metadata":{}},{"cell_type":"code","source":"# Combine dataframes from 2015 to 2020 into a giant dataframe df\ndf = pd.concat([acc_2015, acc_2016, acc_2017, acc_2018, acc_2019, acc_2020])","metadata":{"execution":{"iopub.status.busy":"2022-05-03T18:07:37.609777Z","iopub.execute_input":"2022-05-03T18:07:37.610213Z","iopub.status.idle":"2022-05-03T18:07:37.918613Z","shell.execute_reply.started":"2022-05-03T18:07:37.610138Z","shell.execute_reply":"2022-05-03T18:07:37.917572Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Take df and group the rows by DATE and count up the number of accidents per day from 2015-2020\ngrouped_df = df.groupby(['DATE']).DATE.count().to_frame().rename(columns={'DATE':'num_acc'})\ngrouped_df = grouped_df.to_period('D')\n\nax = grouped_df.plot(figsize=(20,6))\nax.set(title='Fatal Traffic Accidents', ylabel='Number of accidents')","metadata":{"execution":{"iopub.status.busy":"2022-05-03T18:07:39.170456Z","iopub.execute_input":"2022-05-03T18:07:39.170702Z","iopub.status.idle":"2022-05-03T18:07:39.594797Z","shell.execute_reply.started":"2022-05-03T18:07:39.170678Z","shell.execute_reply":"2022-05-03T18:07:39.594183Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Predict and Forecast using Time Dummy","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.deterministic import DeterministicProcess\nfrom sklearn.linear_model import LinearRegression\n\n# Let's add a time series feature -- time dummy\ndp = DeterministicProcess(\n    index=grouped_df.index,  # dates from the training data\n    constant=True,       # dummy feature for the bias (y_intercept)\n    order=1,             # the time dummy (trend)\n    drop=True,           # drop terms if necessary to avoid collinearity\n)\n\nX_time = dp.in_sample()\ny_time = grouped_df[\"num_acc\"]  # the target\n\n# The intercept is the same as the `const` feature from DeterministicProcess. LinearRegression behaves badly with duplicated\n# features, so we need to be sure to exclude it here.\nmodel = LinearRegression(fit_intercept=False)\nmodel.fit(X_time, y_time)\n\ny_time_pred = pd.Series(model.predict(X_time), index=X_time.index)\n\n# We want to forecast 90 days after our time series data (i.e. after Dec 31, 2020)\nX_time = dp.out_of_sample(steps=90)\ny_time_fore = pd.Series(model.predict(X_time), index=X_time.index)\n\n# Plot\nax = grouped_df.plot(style=\".-\", color=\"0.5\", title=\"Accidents - Linear Trend\", figsize=(20,6),alpha=0.25)\nax = y_time_pred.plot(ax=ax, linewidth=3, label=\"Trend\")\nax = y_time_fore.plot(ax=ax, linewidth=3, label=\"Trend Forecast\", color=\"C3\")\n_ = ax.legend()\n\nprint(\"The linear regression equation based on time dummy is num_acc = \",model.coef_[0],\" + \",model.coef_[1],'*num_of_day_since_2015-01-01')","metadata":{"execution":{"iopub.status.busy":"2022-05-03T18:07:42.560319Z","iopub.execute_input":"2022-05-03T18:07:42.560578Z","iopub.status.idle":"2022-05-03T18:07:43.471661Z","shell.execute_reply.started":"2022-05-03T18:07:42.560549Z","shell.execute_reply":"2022-05-03T18:07:43.470785Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Predict Using Lag Feature","metadata":{}},{"cell_type":"code","source":"# Add lag feature\ngrouped_lag = grouped_df.copy()\ngrouped_lag['Lag_1'] = grouped_lag['num_acc'].shift(1)\ngrouped_lag = grouped_lag.reindex(columns=['num_acc', 'Lag_1'])\n\n# Define model based on lag feature\nX_lag = grouped_lag.loc[:, ['Lag_1']]\nX_lag.dropna(inplace=True)  # drop missing values in the feature set\ny_lag = grouped_lag.loc[:, 'num_acc']  # create the target\ny_lag, X_lag = y_lag.align(X_lag, join='inner')  # drop corresponding values in target\n\nmodel = LinearRegression()\nmodel.fit(X_lag, y_lag)\n\ny_lag_pred = pd.Series(model.predict(X_lag), index=X_lag.index)\n\n# Plot\nfig, ax = plt.subplots()\nax.plot(X_lag['Lag_1'], y_lag, '.', color='0.25')\nax.plot(X_lag['Lag_1'], y_lag_pred)\nax.set_aspect('equal')\nax.set_ylabel('num_acc')\nax.set_xlabel('Lag_1')\nax.set_title('1-Day Lag Plot of Accidents');\n\nprint('The linear regression equation based on lag feature is num_acc = ',model.intercept_, ' + ', model.coef_[0], '*Lag_1') ","metadata":{"execution":{"iopub.status.busy":"2022-05-03T18:07:45.566107Z","iopub.execute_input":"2022-05-03T18:07:45.566350Z","iopub.status.idle":"2022-05-03T18:07:45.756549Z","shell.execute_reply.started":"2022-05-03T18:07:45.566328Z","shell.execute_reply":"2022-05-03T18:07:45.755815Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Time plot that shows us how our forecasts (trained with time series data) now respond to the behavior of the series in the recent past.\nplt.figure(figsize=(20,6))\nax = y_lag.plot(color=\"red\") # the actual num_acc\nax = y_lag_pred.plot() # the prediction","metadata":{"execution":{"iopub.status.busy":"2022-05-03T18:07:48.984332Z","iopub.execute_input":"2022-05-03T18:07:48.984588Z","iopub.status.idle":"2022-05-03T18:07:49.336977Z","shell.execute_reply.started":"2022-05-03T18:07:48.984563Z","shell.execute_reply":"2022-05-03T18:07:49.335335Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Explore Moving Average Trend","metadata":{}},{"cell_type":"code","source":"moving_average = grouped_df.rolling(\n    window=365,       # 365-day window\n    center=True,      # puts the average at the center of the window\n    min_periods=183,  # choose about half the window size\n).mean()              # compute the mean (could also do median, std, min, max, ...)\n\nax1 = grouped_df.plot(style=\".\", color=\"0.5\",figsize=(20,6))\nax1.set_aspect(10)\nmoving_average.plot(\n    ax=ax1, linewidth=3, title=\"Traffic Accidents - 365-Day Moving Average\", legend=False,\n);\n\nmoving_average = grouped_df.rolling(\n    window=30,       # 365-day window\n    center=True,      # puts the average at the center of the window\n    min_periods=15,  # choose about half the window size\n).mean()              # compute the mean (could also do median, std, min, max, ...)\n\nax2 = grouped_df.plot(style=\".\", color=\"0.5\",figsize=(20,6))\nax2.set_aspect(10)\nmoving_average.plot(\n    ax=ax2, linewidth=3, title=\"Traffic Accidents - 30-Day Moving Average\", legend=False,\n);","metadata":{"execution":{"iopub.status.busy":"2022-05-03T18:07:51.537755Z","iopub.execute_input":"2022-05-03T18:07:51.538040Z","iopub.status.idle":"2022-05-03T18:07:52.038239Z","shell.execute_reply.started":"2022-05-03T18:07:51.538011Z","shell.execute_reply":"2022-05-03T18:07:52.037535Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Explore Seasonality","metadata":{}},{"cell_type":"code","source":"def seasonal_plot(X, y, period, freq, ax=None):\n    if ax is None:\n        _, ax = plt.subplots()\n    palette = sns.color_palette(\"husl\", n_colors=X[period].nunique(),)\n    ax = sns.lineplot(\n        x=freq,\n        y=y,\n        hue=period,\n        data=X,\n        ci=False,\n        ax=ax,\n        palette=palette,\n        legend=False,\n    )\n    ax.set_title(f\"Seasonal Plot ({period}/{freq})\")\n    for line, name in zip(ax.lines, X[period].unique()):\n        y_ = line.get_ydata()[-1]\n        ax.annotate(\n            name,\n            xy=(1, y_),\n            xytext=(6, 0),\n            color=line.get_color(),\n            xycoords=ax.get_yaxis_transform(),\n            textcoords=\"offset points\",\n            size=14,\n            va=\"center\",\n        )\n    return ax\n\n\ndef plot_periodogram(ts, detrend='linear', ax=None):\n    from scipy.signal import periodogram\n    fs = pd.Timedelta(\"1Y\") / pd.Timedelta(\"1D\")\n    freqencies, spectrum = periodogram(\n        ts,\n        fs=fs,\n        detrend=detrend,\n        window=\"boxcar\",\n        scaling='spectrum',\n    )\n    if ax is None:\n        _, ax = plt.subplots()\n    ax.step(freqencies, spectrum, color=\"purple\")\n    ax.set_xscale(\"log\")\n    ax.set_xticks([1, 2, 4, 6, 12, 26, 52, 104])\n    ax.set_xticklabels(\n        [\n            \"Annual (1)\",\n            \"Semiannual (2)\",\n            \"Quarterly (4)\",\n            \"Bimonthly (6)\",\n            \"Monthly (12)\",\n            \"Biweekly (26)\",\n            \"Weekly (52)\",\n            \"Semiweekly (104)\",\n        ],\n        rotation=30,\n    )\n    ax.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n    ax.set_ylabel(\"Variance\")\n    ax.set_title(\"Periodogram\")\n    return ax","metadata":{"execution":{"iopub.status.busy":"2022-05-03T18:07:54.641336Z","iopub.execute_input":"2022-05-03T18:07:54.642538Z","iopub.status.idle":"2022-05-03T18:07:54.658843Z","shell.execute_reply.started":"2022-05-03T18:07:54.642479Z","shell.execute_reply":"2022-05-03T18:07:54.658242Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"X = grouped_df.copy()\n\n# days within a week\nX[\"day\"] = X.index.dayofweek  # the x-axis (freq)\nX[\"week\"] = X.index.week  # the seasonal period (period)\n\n# days within a year\nX[\"dayofyear\"] = X.index.dayofyear\nX[\"year\"] = X.index.year\nfig, (ax0, ax1) = plt.subplots(2, 1, figsize=(11, 6))\nseasonal_plot(X, y=\"num_acc\", period=\"week\", freq=\"day\", ax=ax0)\nseasonal_plot(X, y=\"num_acc\", period=\"year\", freq=\"dayofyear\", ax=ax1);","metadata":{"execution":{"iopub.status.busy":"2022-05-03T18:07:57.539732Z","iopub.execute_input":"2022-05-03T18:07:57.540487Z","iopub.status.idle":"2022-05-03T18:08:05.209867Z","shell.execute_reply.started":"2022-05-03T18:07:57.540445Z","shell.execute_reply":"2022-05-03T18:08:05.208913Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"We'll model weekly season with indicators and annual season with Fourier features.","metadata":{}},{"cell_type":"code","source":"plot_periodogram(grouped_df.num_acc);","metadata":{"execution":{"iopub.status.busy":"2022-05-03T18:08:08.690165Z","iopub.execute_input":"2022-05-03T18:08:08.690452Z","iopub.status.idle":"2022-05-03T18:08:09.121877Z","shell.execute_reply.started":"2022-05-03T18:08:08.690423Z","shell.execute_reply":"2022-05-03T18:08:09.121039Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"The periodogram falls after after annual (1), so we'll use 1 Fourier pair.","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\n\nfourier = CalendarFourier(freq=\"A\", order=1)  # 1 sin/cos pairs for \"A\"nnual seasonality\n\ndp = DeterministicProcess(\n    index=grouped_df.index,\n    constant=True,               # dummy feature for bias (y-intercept)\n    order=1,                     # trend (order 1 means linear)\n    seasonal=True,               # weekly seasonality (indicators)\n    additional_terms=[fourier],  # annual seasonality (fourier)\n    drop=True,                   # drop terms to avoid collinearity\n)\n\nX = dp.in_sample()  # create features for dates in tunnel.index","metadata":{"execution":{"iopub.status.busy":"2022-05-03T18:08:11.185815Z","iopub.execute_input":"2022-05-03T18:08:11.186274Z","iopub.status.idle":"2022-05-03T18:08:11.207275Z","shell.execute_reply.started":"2022-05-03T18:08:11.186243Z","shell.execute_reply":"2022-05-03T18:08:11.206476Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"With our feature set created, we're ready to fit the model and make predictions. We'll add a 90-day forecast to see how our model extrapolates beyond the training data. The code here is the same as that in earlier lessons.","metadata":{}},{"cell_type":"code","source":"y = grouped_df[\"num_acc\"]\n\nmodel = LinearRegression(fit_intercept=False)\n_ = model.fit(X, y)\n\ny_pred = pd.Series(model.predict(X), index=y.index)\nX_fore = dp.out_of_sample(steps=365)\ny_fore = pd.Series(model.predict(X_fore), index=X_fore.index)\n\nax = y.plot(color='0.25', style='.', title=\"Fatal Traffic Accidents - Seasonal Forecast\",figsize=(20,6))\nax = y_pred.plot(ax=ax, label=\"Seasonal\")\nax = y_fore.plot(ax=ax, label=\"Seasonal Forecast\", color='C3')\n_ = ax.legend()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T18:24:23.073930Z","iopub.execute_input":"2022-05-03T18:24:23.074766Z","iopub.status.idle":"2022-05-03T18:24:23.488181Z","shell.execute_reply.started":"2022-05-03T18:24:23.074691Z","shell.execute_reply":"2022-05-03T18:24:23.487766Z"},"trusted":true},"execution_count":19,"outputs":[]}]}